{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two line logging in ML Flow\n",
    "`import mlflow\n",
    "mlflow.sklearn.autolog(log_input_examples=True, log_model_signatures=True)\n",
    "!mlflow ui`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uname_result(system='Darwin', node='Hannas-MacBook-Pro-2.local', release='18.7.0', version='Darwin Kernel Version 18.7.0: Tue Aug 20 16:57:14 PDT 2019; root:xnu-4903.271.2~2/RELEASE_X86_64', machine='x86_64', processor='i386')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "platform.processor()\n",
    "platform.uname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (4.14.3)\n",
      "Requirement already satisfied: six in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from plotly) (1.15.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from plotly) (1.3.3)\n",
      "Requirement already satisfied: wandb in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (0.10.14)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from wandb) (3.14.0)\n",
      "Requirement already satisfied: sentry-sdk>=0.4.0 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from wandb) (0.19.5)\n",
      "Requirement already satisfied: watchdog<0.10.5,>=0.8.3 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from wandb) (0.10.4)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from wandb) (5.0.1)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from wandb) (3.1.12)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from wandb) (1.0.1)\n",
      "Requirement already satisfied: Click>=7.0 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from wandb) (7.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from wandb) (2.8.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from wandb) (5.3.1)\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from wandb) (3.5.4)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from wandb) (2.25.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from wandb) (1.15.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: pathtools>=0.1.1 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from watchdog<0.10.5,>=0.8.3->wandb) (0.1.2)\n",
      "Requirement already satisfied: matplotlib in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (3.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from matplotlib) (8.1.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from matplotlib) (1.19.5)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six in /anaconda3/envs/guild-ai/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# required to log interactive plots\n",
    "! pip install plotly --upgrade\n",
    "! pip install wandb --upgrade\n",
    "! pip install matplotlib --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression on example of boston data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### need to add saving of clf, env params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import os\n",
    "import numpy as np\n",
    "import platform \n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "# initialize wandb run\n",
    "list_scores = []\n",
    "x_axis = np.linspace(0.1, 0.9, 5)\n",
    "\n",
    "for alpha in x_axis:\n",
    "    wandb.init(project=\"sklearn-regression-boston\", save_code = True, reinit=True)\n",
    "    # Load data\n",
    "    boston = load_boston()\n",
    "    X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "    y = boston.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    namedf = ['X_train', 'X_test']\n",
    "    namenp = ['y_train', 'y_test']\n",
    "\n",
    "    for df, iterdf, arr, iternp in zip([X_train, X_test], namedf, [y_train, y_test], namenp):\n",
    "        #print(str(wandb.run.id))\n",
    "        try:\n",
    "            os.makedirs('wandb/'+str(wandb.run.id)+'/data')\n",
    "        except:\n",
    "            print('Data dir exists')\n",
    "\n",
    "        #print('wandb/'+str(wandb.run.id)+'/data/'+iterdf)\n",
    "        df.to_csv(path_or_buf = 'wandb/'+str(wandb.run.id)+'/data/'+iterdf+\".csv\")\n",
    "        np.savetxt('wandb/'+str(wandb.run.id)+'/data/'+iternp+\".csv\", arr)\n",
    "\n",
    "    reg = Ridge(alpha=alpha)\n",
    "    fitted = reg.fit(X, y)\n",
    "    \n",
    "    wandb.log(reg.get_params())\n",
    "    wandb.log({'test_size':0.2})\n",
    "    wandb.save('sklearn-all-examples.ipynb')\n",
    "    \n",
    "    \n",
    "    # files are automatically saved in each run\n",
    "    # logs all the files in the directory, we can even log the whole directory if necessary by \n",
    "    # doing i.e. wandb.save('*')\n",
    "    wandb.save('wandb/'+str(wandb.run.id)+'/data/*')\n",
    "    \n",
    "    # we don't even need that as it's saved automatically\n",
    "    # wandb.log({\"system\":str(platform.uname())})\n",
    "    \n",
    "    # need to figure out whether it is possible to save the model like in mlflow - if it just dropping like pkl?\n",
    "    # mlflow.sklearn.log_model(fitted, artifact_path=\"model1\")\n",
    "    \n",
    "    # add score\n",
    "    wandb.log({\"score\":fitted.score(X, y)})\n",
    "\n",
    "    list_scores.append(fitted.score(X, y))\n",
    "\n",
    "\n",
    "plt.plot(x_axis, list_scores)\n",
    "#mlflow.log_figure(fig, \"fitted_score.png\")\n",
    "#wandb.log({\"fitted_score\": plt})\n",
    "\n",
    "wandb.log({\"fitted_score\": wandb.Image(plt)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#clf = np.load('mlruns/0/112c057cb37d4cb6b62ca5bed12ee5ff/artifacts/model/model.pkl', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification using Random Forest on the example of iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.sklearn.autolog(log_input_examples=True, log_model_signatures=True)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from numpy.random.mtrand import permutation\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# load data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "labels = iris.target_names\n",
    "features = iris.feature_names\n",
    "\n",
    "y[y != 0] = 1\n",
    "\n",
    "# shuffle data\n",
    "X, y = shuffle(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# create model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X_test)\n",
    "y_probas = model.predict_proba(X_test)\n",
    "\n",
    "import scikitplot\n",
    "scikitplot.metrics.plot_roc(y_test, y_probas);\n",
    "scikitplot.metrics.plot_precision_recall(y_test, y_probas);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with KMeans\n",
    "## using iris data\n",
    "(can add also DBSCAN, AgglomerativeClustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.sklearn.autolog(log_input_examples=True, log_model_signatures=True)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn import datasets, cluster\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "# load data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "names = iris.target_names\n",
    "\n",
    "def get_label_ids(classes):\n",
    "    return np.array([names[aclass] for aclass in classes])\n",
    "labels = get_label_ids(y)\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=1)\n",
    "cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "scikitplot.cluster.plot_elbow_curve(kmeans, X)\n",
    "scikitplot.metrics.plot_silhouette(X, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM on iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.sklearn.autolog(log_input_examples=True, log_model_signatures=True)\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import sklearn\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "X_combined_std = np.vstack((X_train_std, X_test_std))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "\n",
    "# Fit model\n",
    "svm = SVC(kernel='rbf', random_state=0, gamma=10, C=1)\n",
    "svm.fit(X_train_std, y_train)\n",
    "\n",
    "# Create a matplotlib custom plot to save \n",
    "def plot_data():\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    markers = ('s', 'x', 'o')\n",
    "    colors = ('red', 'blue', 'lightgreen')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y_test))])\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "               c=cmap(idx), marker=markers[idx], label=cl)\n",
    "\n",
    "\n",
    "plot_data()\n",
    "\n",
    "#### get the whole list of the parameters for the run for the estimator (if we are not logging it inside the loop)param_config = svm.get_params()\n",
    "param_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulsar stars detection: clustering, classification, ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from itertools import cycle, islice\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Load data\n",
    "pulsar = pd.read_csv('../data/pulsar_stars.csv')\n",
    "\n",
    "# Get numeric labels for each of the string labels, to make them compatible with our model\n",
    "labels_to_class = {'Pulsar': 0, 'Not a Pulsar': 1}\n",
    "def get_class_ids(labels):\n",
    "    return np.array([labels_to_class[alabel] for alabel in labels])\n",
    "def get_named_labels(labels, numeric_labels):\n",
    "        return np.array([labels[num_label] for num_label in numeric_labels])\n",
    "\n",
    "# Remove target variables label (and class)\n",
    "features = list(set(pulsar.columns) - {'target_class'})\n",
    "X = pulsar[features]\n",
    "y = pulsar['target_class']\n",
    "labels = ['Pulsar', 'Not a Pulsar']\n",
    "X = X[:50000]\n",
    "y = y[:50000]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "features_scaled = scaler.fit_transform(X)\n",
    "# Split into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False)\n",
    "\n",
    "# Clustering - predict particle clusters without labels\n",
    "# KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=1)\n",
    "cluster_labels = kmeans.fit_predict(X_train)\n",
    "label_names = get_named_labels(labels, cluster_labels)\n",
    "\n",
    "# Classification - predict pulsar\n",
    "# Train a model, get predictions\n",
    "log = lm.LogisticRegression(random_state=4)\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "dtree = DecisionTreeClassifier(random_state=4)\n",
    "rtree = RandomForestClassifier(n_estimators=100, random_state=4)\n",
    "svm = SVC(random_state=4, probability=True)\n",
    "nb = GaussianNB()\n",
    "gbc = GradientBoostingClassifier()\n",
    "adaboost = AdaBoostClassifier(n_estimators=500, learning_rate=0.01, random_state=42,\n",
    "                             base_estimator=DecisionTreeClassifier(max_depth=8,\n",
    "                             min_samples_leaf=10, random_state=42))\n",
    "\n",
    "def model_algorithm(clf, X_train, y_train, X_test, y_test, name, labels, features):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_probas = clf.predict_proba(X_test)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "model_algorithm(log, X_train, y_train, X_test, y_test, 'LogisticRegression', labels, features)\n",
    "model_algorithm(knn, X_train, y_train, X_test, y_test, 'KNearestNeighbor', labels, features)\n",
    "model_algorithm(dtree, X_train, y_train, X_test, y_test, 'DecisionTree', labels, features)\n",
    "model_algorithm(rtree, X_train, y_train, X_test, y_test, 'RandomForest', labels, features)\n",
    "model_algorithm(svm, X_train, y_train, X_test, y_test, 'SVM', labels, features)\n",
    "model_algorithm(nb, X_train, y_train, X_test, y_test, 'NaiveBayes', labels, features)\n",
    "model_algorithm(adaboost, X_train, y_train, X_test, y_test, 'AdaBoost', labels, features)\n",
    "model_algorithm(gbc, X_train, y_train, X_test, y_test, 'GradientBoosting', labels, features)\n",
    "\n",
    "# Regression - TrackP - particle momentum\n",
    "features = list(set(pulsar.columns) - {' Mean of the integrated profile'})\n",
    "X = pulsar[features]\n",
    "y = pulsar[' Mean of the integrated profile']\n",
    "X = X[:10000]\n",
    "y = y[:10000]\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size=0.90, test_size=0.10)\n",
    "\n",
    "# Train a model, get predictions\n",
    "reg = Ridge()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic data: clustering, classification and regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from itertools import cycle, islice\n",
    "from sklearn.neighbors import BallTree, KDTree, DistanceMetric\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Load data\n",
    "pulsar = pd.read_csv('../data/titanic-train.csv')\n",
    "\n",
    "# Get numeric labels for each of the string labels, to make them compatible with our model\n",
    "labels_to_class = {'Did not Survive': 0, 'Survived': 1}\n",
    "def get_class_ids(labels):\n",
    "    return np.array([labels_to_class[alabel] for alabel in labels])\n",
    "def get_named_labels(labels, numeric_labels):\n",
    "        return np.array([labels[num_label] for num_label in numeric_labels])\n",
    "\n",
    "# Remove target variables label (and class)\n",
    "features = list(set(pulsar.columns) - {'Survived','Name'})\n",
    "X = pulsar[features]\n",
    "y = pulsar['Survived']\n",
    "labels = ['Did not Survive', 'Survived']\n",
    "X = X[:50000]\n",
    "X = X.replace(\"\", np.nan, regex = True)\n",
    "y = y[:50000]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "features_scaled = scaler.fit_transform(X)\n",
    "# Split into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False)\n",
    "\n",
    "# Clustering - predict particle clusters without labels\n",
    "# KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=1)\n",
    "cluster_labels = kmeans.fit_predict(X_train)\n",
    "label_names = get_named_labels(labels, cluster_labels)\n",
    "\n",
    "# Classification - predict pulsar\n",
    "# Train a model, get predictions\n",
    "log = lm.LogisticRegression(random_state=4)\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "dtree = DecisionTreeClassifier(random_state=4)\n",
    "rtree = RandomForestClassifier(n_estimators=100, random_state=4)\n",
    "svm = SVC(random_state=4, probability=True)\n",
    "nb = GaussianNB()\n",
    "gbc = GradientBoostingClassifier()\n",
    "adaboost = AdaBoostClassifier(n_estimators=500, learning_rate=0.01, random_state=42,\n",
    "                             base_estimator=DecisionTreeClassifier(max_depth=8,\n",
    "                             min_samples_leaf=10, random_state=42))\n",
    "\n",
    "def model_algorithm(clf, X_train, y_train, X_test, y_test, name, labels, features):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_probas = clf.predict_proba(X_test)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "model_algorithm(log, X_train, y_train, X_test, y_test, 'LogisticRegression', labels, features)\n",
    "model_algorithm(knn, X_train, y_train, X_test, y_test, 'KNearestNeighbor', labels, features)\n",
    "model_algorithm(dtree, X_train, y_train, X_test, y_test, 'DecisionTree', labels, features)\n",
    "model_algorithm(rtree, X_train, y_train, X_test, y_test, 'RandomForest', labels, features)\n",
    "model_algorithm(svm, X_train, y_train, X_test, y_test, 'SVM', labels, features)\n",
    "model_algorithm(nb, X_train, y_train, X_test, y_test, 'NaiveBayes', labels, features)\n",
    "model_algorithm(adaboost, X_train, y_train, X_test, y_test, 'AdaBoost', labels, features)\n",
    "model_algorithm(gbc, X_train, y_train, X_test, y_test, 'GradientBoosting', labels, features)\n",
    "\n",
    "# Regression - TrackP - particle momentum\n",
    "features = list(set(pulsar.columns) - {'Age'})\n",
    "X = pulsar[features]\n",
    "y = pulsar['Age']\n",
    "X = X[:10000]\n",
    "y = y[:10000]\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size=0.90, test_size=0.10)\n",
    "\n",
    "# Train a model, get predictions\n",
    "reg = Ridge()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier on tweets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "        \n",
    "def plot(nb, X_test, y_test, y_probas, y_pred):\n",
    "        scikitplot.estimators.plot_learning_curve(nb, X_test, y_test)\n",
    "        scikitplot.metrics.plot_roc(y_test, y_probas)\n",
    "        scikitplot.metrics.plot_confusion_matrix(y_test, y_pred, labels=nb.classes_)\n",
    "        scikitplot.metrics.plot_precision_recall(y_test, y_probas)\n",
    "\n",
    "\n",
    "def tweets():\n",
    "\n",
    "    # Get a pandas DataFrame object of all the data in the csv file:\n",
    "    df = pd.read_csv('../data/tweets.csv')\n",
    "\n",
    "    # Get pandas Series object of the \"tweet text\" column:\n",
    "    text = df['tweet_text']\n",
    "\n",
    "    # Get pandas Series object of the \"emotion\" column:\n",
    "    target = df['is_there_an_emotion_directed_at_a_brand_or_product']\n",
    "\n",
    "    # Remove the blank rows from the series:\n",
    "    target = target[pd.notnull(text)]\n",
    "    text = text[pd.notnull(text)]\n",
    "\n",
    "    # Perform feature extraction:\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    count_vect = CountVectorizer()\n",
    "    count_vect.fit(text)\n",
    "    counts = count_vect.transform(text)\n",
    "\n",
    "\n",
    "    counts_train = counts[:6000]\n",
    "    target_train = target[:6000]\n",
    "    counts_test = counts[6000:]\n",
    "    target_test = target[6000:]\n",
    "\n",
    "\n",
    "    # Train with this data with a Naive Bayes classifier:\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(counts, target)\n",
    "\n",
    "\n",
    "    X_test = counts_test\n",
    "    y_test = target_test\n",
    "    y_probas = nb.predict_proba(X_test)\n",
    "    y_pred = nb.predict(X_test)\n",
    "\n",
    "    print(\"y\", y_probas.shape)\n",
    "    plot(nb, X_test, y_test, y_probas, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:guild-ai] *",
   "language": "python",
   "name": "conda-env-guild-ai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
